# 5. GOVERNANCE AUTOMATION: COMPLIANCE AS CODE

## 5.1 Automated PII Masking & Data Protection

### 5.1.1 Multi-Layer Masking Strategy

```
LAYER 1: DATABASE LEVEL (SQL)
â”œâ”€ Automatic trigger-based masking
â”œâ”€ Column-level encryption
â””â”€ Applied at data load time

LAYER 2: APPLICATION LEVEL (Python)
â”œâ”€ Masking before test execution
â”œâ”€ Field-type aware anonymization
â””â”€ Reversible masking for debugging

LAYER 3: TRANSPORT LEVEL (TLS)
â”œâ”€ Encrypted data in transit
â”œâ”€ Vault-managed secrets
â””â”€ No plain-text logs

LAYER 4: AUDIT LEVEL (Immutable Logs)
â”œâ”€ Track all data access
â”œâ”€ Immutable audit trail
â””â”€ Compliance proof
```

### 5.1.2 Database-Level PII Masking

```sql
-- governance/pii_masking_rules.sql
-- Applied immediately after data provisioning
-- Transparent to applications; tests see masked data

-- Create masking function for emails
CREATE OR REPLACE FUNCTION mask_email(email_val VARCHAR) RETURNS VARCHAR AS $$
BEGIN
    RETURN 'masked-' || MD5(email_val) || '@example.com';
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- Create masking function for phone numbers
CREATE OR REPLACE FUNCTION mask_phone(phone_val VARCHAR) RETURNS VARCHAR AS $$
BEGIN
    RETURN '555-' || LPAD((RANDOM() * 9999)::int::text, 4, '0');
END;
$$ LANGUAGE plpgsql;

-- Create masking function for SSN
CREATE OR REPLACE FUNCTION mask_ssn(ssn_val VARCHAR) RETURNS VARCHAR AS $$
BEGIN
    RETURN 'XXX-XX-' || SUBSTRING(ssn_val, -4);
END;
$$ LANGUAGE plpgsql IMMUTABLE;

-- Apply masking to users table
UPDATE users SET
    email = mask_email(email),
    phone = mask_phone(phone),
    ssn = mask_ssn(ssn),
    address = 'Masked Address, Masked City, Masked State 00000',
    date_of_birth = DATE_TRUNC('year', date_of_birth)::date
WHERE is_test_data = true;

-- Apply masking to payment methods
UPDATE payment_methods SET
    card_number = 'XXXX-XXXX-XXXX-' || SUBSTRING(card_number, -4),
    cardholder_name = 'MASKED NAME',
    cvv = 'XXX'
WHERE is_test_data = true;

-- Apply masking to user activity logs
UPDATE user_activity_logs SET
    ip_address = '192.168.1.1',
    user_agent = 'Mozilla/5.0 (Generic Browser)',
    session_data = jsonb_set(session_data, '{location}', '"Masked"'::jsonb)
WHERE created_date >= NOW() - INTERVAL '24 hours'
  AND environment = 'test';

-- Verify masking applied
SELECT 
    table_name,
    COUNT(*) as total_rows,
    COUNT(CASE WHEN email LIKE '%@example.com' THEN 1 END) as masked_emails,
    COUNT(CASE WHEN phone LIKE '555-%' THEN 1 END) as masked_phones
FROM information_schema.tables t
INNER JOIN users u ON u.table_name = t.table_name
WHERE is_test_data = true
GROUP BY table_name;

-- Create audit event for masking
INSERT INTO audit_log (event_type, resource_type, action, details, timestamp)
VALUES (
    'PII_MASKING_APPLIED',
    'test_data',
    'AUTOMATIC_MASKING',
    jsonb_build_object(
        'tables_masked', ARRAY['users', 'payment_methods', 'user_activity_logs'],
        'masking_rules_applied', ARRAY['email', 'phone', 'ssn', 'card_number'],
        'records_masked', (SELECT COUNT(*) FROM users WHERE is_test_data = true)
    ),
    NOW()
);
```

---

## 5.2 Role-Based Access Control (RBAC) for Test Data

### 5.2.1 Service Account Permissions

```python
# governance/rbac_enforcement.py

from enum import Enum
from dataclasses import dataclass
from typing import List, Set
import json

class Role(Enum):
    """Roles for test environment access"""
    DEVELOPER = "developer"          # Can provision data, run tests locally
    QA_AUTOMATION = "qa-automation"  # Can execute and manage test pipelines
    QA_LEAD = "qa-lead"             # Can manage test data, approve test suites
    DEVOPS = "devops"               # Can manage infrastructure, secrets
    SECURITY = "security"           # Read-only audit logs
    ADMIN = "admin"                 # Full access

class Permission(Enum):
    """Fine-grained permissions"""
    # Data Access
    DATA_READ = "data:read"
    DATA_CREATE = "data:create"
    DATA_UPDATE = "data:update"
    DATA_DELETE = "data:delete"
    DATA_EXPORT = "data:export"
    
    # Environment
    ENV_CREATE = "env:create"
    ENV_MODIFY = "env:modify"
    ENV_DELETE = "env:delete"
    ENV_VIEW = "env:view"
    
    # Pipeline
    PIPELINE_EXECUTE = "pipeline:execute"
    PIPELINE_APPROVE = "pipeline:approve"
    PIPELINE_VIEW = "pipeline:view"
    
    # Audit
    AUDIT_READ = "audit:read"
    AUDIT_EXPORT = "audit:export"
    
    # Secrets
    SECRET_READ = "secret:read"
    SECRET_ROTATE = "secret:rotate"

@dataclass
class RoleDefinition:
    """Defines permissions for each role"""
    name: Role
    permissions: Set[Permission]
    resource_constraints: dict

# Role definitions
ROLE_PERMISSIONS = {
    Role.DEVELOPER: RoleDefinition(
        name=Role.DEVELOPER,
        permissions={
            Permission.DATA_READ,
            Permission.DATA_CREATE,
            Permission.ENV_CREATE,
            Permission.ENV_VIEW,
            Permission.PIPELINE_EXECUTE,
            Permission.AUDIT_READ,
        },
        resource_constraints={
            "environment": {"owner": "${user}"},  # Only own environments
            "pipeline": {"branch": ["develop", "feature/*"]},
        }
    ),
    Role.QA_AUTOMATION: RoleDefinition(
        name=Role.QA_AUTOMATION,
        permissions={
            Permission.DATA_READ,
            Permission.DATA_CREATE,
            Permission.DATA_DELETE,
            Permission.ENV_CREATE,
            Permission.ENV_MODIFY,
            Permission.ENV_DELETE,
            Permission.PIPELINE_EXECUTE,
            Permission.PIPELINE_APPROVE,
            Permission.AUDIT_READ,
        },
        resource_constraints={
            "environment": {"namespace": "test"},
            "pipeline": {"branch": ["*"]},
        }
    ),
    Role.QA_LEAD: RoleDefinition(
        name=Role.QA_LEAD,
        permissions={
            Permission.DATA_READ,
            Permission.DATA_CREATE,
            Permission.DATA_UPDATE,
            Permission.DATA_DELETE,
            Permission.DATA_EXPORT,
            Permission.ENV_CREATE,
            Permission.ENV_MODIFY,
            Permission.ENV_DELETE,
            Permission.PIPELINE_EXECUTE,
            Permission.PIPELINE_APPROVE,
            Permission.AUDIT_READ,
            Permission.AUDIT_EXPORT,
        },
        resource_constraints={}
    ),
    Role.DEVOPS: RoleDefinition(
        name=Role.DEVOPS,
        permissions={
            Permission.ENV_CREATE,
            Permission.ENV_MODIFY,
            Permission.ENV_DELETE,
            Permission.ENV_VIEW,
            Permission.SECRET_READ,
            Permission.SECRET_ROTATE,
            Permission.AUDIT_READ,
        },
        resource_constraints={
            "environment": {"type": ["infrastructure", "secrets"]},
        }
    ),
    Role.SECURITY: RoleDefinition(
        name=Role.SECURITY,
        permissions={
            Permission.DATA_READ,
            Permission.AUDIT_READ,
            Permission.AUDIT_EXPORT,
            Permission.ENV_VIEW,
            Permission.PIPELINE_VIEW,
        },
        resource_constraints={}
    ),
    Role.ADMIN: RoleDefinition(
        name=Role.ADMIN,
        permissions=set(Permission),  # All permissions
        resource_constraints={}
    ),
}

class RBACEnforcer:
    """Enforces RBAC policies in pipeline"""
    
    def __init__(self):
        self.user_roles = {}  # Map of user -> roles
        self.audit_log = []
    
    def check_permission(self, user: str, permission: Permission, resource: dict = None) -> bool:
        """
        Check if user has permission for action on resource.
        
        Usage:
        if rbac.check_permission("alice@company.com", Permission.DATA_CREATE, 
                                 {"environment": "test-e2e"}):
            # Proceed with data creation
        """
        user_roles = self.user_roles.get(user, [Role.DEVELOPER])
        
        for role in user_roles:
            role_def = ROLE_PERMISSIONS.get(role)
            
            # Check if role has permission
            if permission not in role_def.permissions:
                continue
            
            # Check resource constraints
            if resource and not self._check_constraints(role_def.resource_constraints, resource):
                continue
            
            # Permission granted
            self._log_access(user, permission, resource, "ALLOWED")
            return True
        
        # Permission denied
        self._log_access(user, permission, resource, "DENIED")
        return False
    
    def _check_constraints(self, constraints: dict, resource: dict) -> bool:
        """Check if resource matches constraints"""
        for constraint_key, constraint_val in constraints.items():
            if constraint_key not in resource:
                return False
            
            # Support wildcards
            if isinstance(constraint_val, list):
                if not any(self._match_pattern(str(resource[constraint_key]), p) 
                          for p in constraint_val):
                    return False
            else:
                # Support template variables like ${user}
                if not self._match_constraint(constraint_val, resource[constraint_key]):
                    return False
        
        return True
    
    def _match_pattern(self, value: str, pattern: str) -> bool:
        """Match value against pattern (supports wildcards)"""
        import fnmatch
        return fnmatch.fnmatch(value, pattern)
    
    def _match_constraint(self, constraint: any, value: any) -> bool:
        """Match constraint value"""
        return str(constraint) == str(value)
    
    def _log_access(self, user: str, permission: Permission, resource: dict, decision: str):
        """Log access control decision"""
        from datetime import datetime
        
        self.audit_log.append({
            "timestamp": datetime.utcnow().isoformat(),
            "user": user,
            "permission": permission.value,
            "resource": resource or {},
            "decision": decision
        })

# Usage in pipeline
def check_pipeline_permission(user_email: str, action: str):
    """
    Check if user can perform action in CI/CD pipeline.
    Called before allowing pipeline execution.
    """
    enforcer = RBACEnforcer()
    enforcer.user_roles[user_email] = [Role.QA_AUTOMATION]  # Loaded from IdP
    
    resource = {
        "environment": "test",
        "branch": "main",
        "pipeline": "e2e-tests"
    }
    
    if action == "execute" and Permission.PIPELINE_EXECUTE:
        if not enforcer.check_permission(user_email, Permission.PIPELINE_EXECUTE, resource):
            raise PermissionError(f"{user_email} cannot execute pipelines")
    
    if action == "approve" and Permission.PIPELINE_APPROVE:
        if not enforcer.check_permission(user_email, Permission.PIPELINE_APPROVE, resource):
            raise PermissionError(f"{user_email} cannot approve pipelines")
```

### 5.2.2 Kubernetes RBAC Integration

```yaml
# governance/k8s-rbac.yaml
# Kubernetes ServiceAccount + RBAC for test environments

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-data-processor
  namespace: test-automation

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: test-data-processor-role
  namespace: test-automation
rules:
# Allow reading secrets for database credentials
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get", "list"]
  resourceNames: ["postgres-credentials", "api-credentials"]

# Allow creating/deleting temporary PVCs for test data
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["create", "delete", "list"]

# Allow reading ConfigMaps for test configuration
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get", "list"]

# Allow writing logs
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get", "list"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: test-data-processor-rolebinding
  namespace: test-automation
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: test-data-processor-role
subjects:
- kind: ServiceAccount
  name: test-data-processor
  namespace: test-automation

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: test-executor
  namespace: test-automation

---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: test-executor-role
  namespace: test-automation
rules:
# Allow running tests (create pods)
- apiGroups: [""]
  resources: ["pods"]
  verbs: ["create", "get", "list", "watch"]

# Allow accessing logs
- apiGroups: [""]
  resources: ["pods/log"]
  verbs: ["get"]

# Allow creating temporary volumes
- apiGroups: [""]
  resources: ["persistentvolumeclaims"]
  verbs: ["create", "get", "delete"]

# Deny data modification (read-only)
- apiGroups: [""]
  resources: ["secrets"]
  verbs: ["get"]
  resourceNames: ["test-db-ro"]  # Read-only credentials

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: test-executor-rolebinding
  namespace: test-automation
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: test-executor-role
subjects:
- kind: ServiceAccount
  name: test-executor
  namespace: test-automation
```

---

## 5.3 Audit Logging & Immutable Records

### 5.3.1 Comprehensive Audit Logger

```python
# governance/audit_logger.py

from dataclasses import dataclass, asdict
from datetime import datetime
from typing import Dict, Any, Optional
import json
import logging
import hashlib
from enum import Enum

logger = logging.getLogger(__name__)

class AuditEventType(Enum):
    """Types of audit events"""
    # Data Access
    DATA_PROVISIONED = "data_provisioned"
    DATA_ACCESSED = "data_accessed"
    DATA_MASKED = "data_masked"
    DATA_DELETED = "data_deleted"
    
    # Environment
    ENV_CREATED = "env_created"
    ENV_MODIFIED = "env_modified"
    ENV_DESTROYED = "env_destroyed"
    ENV_ACCESSED = "env_accessed"
    
    # Pipeline
    PIPELINE_EXECUTED = "pipeline_executed"
    PIPELINE_FAILED = "pipeline_failed"
    PIPELINE_APPROVED = "pipeline_approved"
    
    # Security
    RBAC_CHECK_PASSED = "rbac_check_passed"
    RBAC_CHECK_FAILED = "rbac_check_failed"
    SECRET_ACCESSED = "secret_accessed"
    SECRET_ROTATED = "secret_rotated"
    
    # Compliance
    COMPLIANCE_SCAN = "compliance_scan"
    POLICY_VIOLATION = "policy_violation"

@dataclass
class AuditEvent:
    """Immutable audit event record"""
    event_type: AuditEventType
    timestamp: str
    actor: str  # User or system principal
    action: str
    resource_type: str
    resource_id: str
    resource_details: Dict[str, Any]
    result: str  # "success", "failure", "denied"
    error_message: Optional[str] = None
    client_ip: Optional[str] = None
    request_id: str = None
    
    def to_json(self) -> str:
        """Serialize to JSON"""
        event_dict = asdict(self)
        event_dict['event_type'] = event_dict['event_type'].value
        return json.dumps(event_dict, default=str)
    
    def compute_hash(self) -> str:
        """Compute SHA256 hash for tamper detection"""
        json_str = self.to_json()
        return hashlib.sha256(json_str.encode()).hexdigest()

class AuditLogger:
    """Centralized audit logging with immutability guarantees"""
    
    def __init__(self, storage_backend: 'StorageBackend'):
        """
        Initialize audit logger.
        
        Args:
            storage_backend: Immutable storage (S3, CloudSQL, etc.)
        """
        self.backend = storage_backend
        self.local_buffer = []
        self.previous_hash = None
    
    def log_event(
        self,
        event_type: AuditEventType,
        actor: str,
        action: str,
        resource_type: str,
        resource_id: str,
        resource_details: Dict[str, Any],
        result: str,
        error_message: str = None,
        client_ip: str = None,
        request_id: str = None
    ) -> AuditEvent:
        """
        Log an audit event with tamper-proof chain of custody.
        """
        event = AuditEvent(
            event_type=event_type,
            timestamp=datetime.utcnow().isoformat() + "Z",
            actor=actor,
            action=action,
            resource_type=resource_type,
            resource_id=resource_id,
            resource_details=resource_details,
            result=result,
            error_message=error_message,
            client_ip=client_ip,
            request_id=request_id or self._generate_request_id()
        )
        
        # Compute hash for chain of custody
        event_hash = event.compute_hash()
        
        # Add to immutable storage
        self.backend.write(
            table="audit_log",
            event=event.to_json(),
            event_hash=event_hash,
            previous_hash=self.previous_hash
        )
        
        # Update chain pointer
        self.previous_hash = event_hash
        
        # Buffer locally for performance
        self.local_buffer.append(event)
        
        # Log to application logger as well
        logger.info(
            f"[AUDIT] {event_type.value}: {actor} {action} {resource_type}/{resource_id} -> {result}",
            extra={
                "event_type": event_type.value,
                "actor": actor,
                "request_id": event.request_id
            }
        )
        
        return event
    
    def log_data_provisioning(
        self,
        actor: str,
        datasets: list,
        volumes: Dict[str, int],
        masking_applied: bool,
        request_id: str = None
    ) -> AuditEvent:
        """Log test data provisioning"""
        return self.log_event(
            event_type=AuditEventType.DATA_PROVISIONED,
            actor=actor,
            action="PROVISION",
            resource_type="test_data",
            resource_id=request_id or self._generate_request_id(),
            resource_details={
                "datasets": datasets,
                "volumes": volumes,
                "masking_applied": masking_applied
            },
            result="success"
        )
    
    def log_environment_creation(
        self,
        actor: str,
        environment_id: str,
        environment_details: Dict[str, Any],
        request_id: str = None
    ) -> AuditEvent:
        """Log environment creation"""
        return self.log_event(
            event_type=AuditEventType.ENV_CREATED,
            actor=actor,
            action="CREATE",
            resource_type="test_environment",
            resource_id=environment_id,
            resource_details=environment_details,
            result="success",
            request_id=request_id
        )
    
    def log_rbac_check(
        self,
        actor: str,
        permission: str,
        resource: str,
        granted: bool,
        request_id: str = None
    ) -> AuditEvent:
        """Log RBAC access control check"""
        return self.log_event(
            event_type=AuditEventType.RBAC_CHECK_PASSED if granted else AuditEventType.RBAC_CHECK_FAILED,
            actor=actor,
            action="ACCESS_CHECK",
            resource_type="access_control",
            resource_id=resource,
            resource_details={
                "permission": permission,
                "granted": granted
            },
            result="success" if granted else "denied",
            request_id=request_id
        )
    
    def query_audit_log(
        self,
        actor: str = None,
        event_type: AuditEventType = None,
        resource_type: str = None,
        start_time: datetime = None,
        end_time: datetime = None
    ) -> list:
        """Query audit log with filtering"""
        return self.backend.query(
            table="audit_log",
            filters={
                "actor": actor,
                "event_type": event_type.value if event_type else None,
                "resource_type": resource_type,
                "timestamp__gte": start_time.isoformat() if start_time else None,
                "timestamp__lte": end_time.isoformat() if end_time else None,
            }
        )
    
    def verify_chain_of_custody(self) -> bool:
        """
        Verify audit log integrity by checking hash chain.
        Returns True if no tampering detected.
        """
        events = self.backend.query(table="audit_log", order_by="timestamp")
        
        previous_hash = None
        for event_record in events:
            event_hash = event_record['event_hash']
            stored_previous_hash = event_record['previous_hash']
            
            # Verify chain
            if previous_hash != stored_previous_hash:
                logger.error(f"Hash chain broken at event {event_record['timestamp']}")
                return False
            
            previous_hash = event_hash
        
        logger.info("âœ“ Audit log integrity verified")
        return True
    
    def _generate_request_id(self) -> str:
        """Generate unique request ID"""
        import uuid
        return str(uuid.uuid4())

# Storage backends
class S3AuditBackend:
    """Store immutable audit logs in S3 (append-only bucket)"""
    
    def __init__(self, bucket_name: str, region: str = "us-east-1"):
        import boto3
        self.s3 = boto3.client("s3", region_name=region)
        self.bucket = bucket_name
    
    def write(self, table: str, event: str, event_hash: str, previous_hash: str):
        """Write event to immutable S3"""
        import json
        from datetime import datetime
        
        timestamp = datetime.utcnow().strftime("%Y/%m/%d/%H/%M/%S")
        key = f"{table}/{timestamp}/{event_hash}.json"
        
        record = {
            "event": json.loads(event),
            "event_hash": event_hash,
            "previous_hash": previous_hash
        }
        
        self.s3.put_object(
            Bucket=self.bucket,
            Key=key,
            Body=json.dumps(record),
            ServerSideEncryption="AES256",
            Tagging="audit=true&immutable=true"
        )

class CloudSQLAuditBackend:
    """Store immutable audit logs in Cloud SQL with triggers"""
    
    def __init__(self, db_url: str):
        import psycopg2
        self.conn = psycopg2.connect(db_url)
    
    def write(self, table: str, event: str, event_hash: str, previous_hash: str):
        """Write to Cloud SQL with append-only constraints"""
        cursor = self.conn.cursor()
        
        cursor.execute(f"""
            INSERT INTO {table} (event, event_hash, previous_hash, created_at)
            VALUES (%s, %s, %s, NOW())
        """, (event, event_hash, previous_hash))
        
        self.conn.commit()

# Usage
if __name__ == "__main__":
    # Initialize audit logger with S3 backend
    backend = S3AuditBackend(bucket_name="audit-logs-immutable")
    audit_logger = AuditLogger(backend)
    
    # Log data provisioning
    audit_logger.log_data_provisioning(
        actor="pipeline@ci.example.com",
        datasets=["users", "orders"],
        volumes={"users": 100, "orders": 500},
        masking_applied=True
    )
    
    # Log environment creation
    audit_logger.log_environment_creation(
        actor="devops@example.com",
        environment_id="test-env-12345",
        environment_details={
            "region": "us-east-1",
            "vpc_id": "vpc-12345",
            "services": ["postgres", "redis", "api"]
        }
    )
    
    # Log RBAC check
    audit_logger.log_rbac_check(
        actor="alice@example.com",
        permission="data:create",
        resource="test_data",
        granted=True
    )
```

---

## 5.4 Policy-as-Code Enforcement

### 5.4.1 OPA (Open Policy Agent) Policies

```rego
# governance/policies/test_data_governance.rego
# Enforces test data governance policies using OPA

package test_automation.data_governance

import future.keywords

# Policy: Data provisioning requires masking
deny[msg] {
    input.action == "provision_data"
    input.masking_enabled != true
    msg := "Data provisioning requires PII masking to be enabled"
}

# Policy: Test data cannot access production credentials
deny[msg] {
    input.environment == "test"
    input.credentials.source == "production"
    msg := "Test environments cannot use production credentials"
}

# Policy: Data volumes must be reasonable
deny[msg] {
    input.action == "provision_data"
    input.volumes.users > 10000
    msg := sprintf("User data volume %d exceeds maximum 10000", [input.volumes.users])
}

# Policy: Only authorized roles can delete test data
deny[msg] {
    input.action == "delete_data"
    not input.user_roles[_] in ["admin", "qa-lead"]
    msg := "Only admins or QA leads can delete test data"
}

# Policy: RBAC must be enforced for all environment access
deny[msg] {
    input.action in ["create_env", "modify_env", "delete_env"]
    not input.rbac_enforced
    msg := "RBAC enforcement required for environment operations"
}

# Policy: Audit logging must be enabled
deny[msg] {
    input.environment == "test"
    input.audit_logging_enabled != true
    msg := "Audit logging must be enabled for compliance"
}

# Policy: Data retention must comply with GDPR
deny[msg] {
    input.action == "provision_data"
    input.retention_days > 90
    msg := "Test data retention cannot exceed 90 days per GDPR compliance"
}

# Allowed masking algorithms
allowed_masking_algorithms := ["sha256_hash", "anonymize_email", "redact_phone", "randomize_address"]

deny[msg] {
    input.action == "provision_data"
    masking_rule := input.masking_rules[_]
    not masking_rule.algorithm in allowed_masking_algorithms
    msg := sprintf("Masking algorithm '%s' not in allowed list", [masking_rule.algorithm])
}

# Check all required governance controls
check_governance_controls[result] {
    result := {
        "masking_enabled": input.masking_enabled == true,
        "audit_logging": input.audit_logging_enabled == true,
        "rbac_enforced": input.rbac_enforced == true,
        "encryption_enabled": input.encryption_enabled == true,
        "data_retention_valid": input.retention_days <= 90
    }
}
```

### 5.4.2 OPA Integration in Pipeline

```python
# governance/opa_policy_enforcer.py

import json
import subprocess
from typing import Dict, Any, List

class OPAPolicyEnforcer:
    """Enforces Rego-based policies using OPA"""
    
    def __init__(self, opa_binary_path: str = "/usr/local/bin/opa"):
        self.opa_bin = opa_binary_path
    
    def evaluate_policy(
        self,
        policy_file: str,
        input_data: Dict[str, Any],
        package: str = "test_automation.data_governance"
    ) -> Dict[str, Any]:
        """
        Evaluate OPA policy against input data.
        
        Returns:
        {
            "allowed": bool,
            "violations": List[str],
            "details": Dict
        }
        """
        # Prepare input
        opa_input = json.dumps({"input": input_data})
        
        # Run OPA evaluation
        result = subprocess.run([
            self.opa_bin, "eval",
            "-d", policy_file,
            "-p", package,
            "-I",  # Strict mode
        ], input=opa_input, capture_output=True, text=True)
        
        if result.returncode != 0:
            return {
                "allowed": False,
                "violations": ["OPA evaluation failed"],
                "error": result.stderr
            }
        
        # Parse output
        opa_result = json.loads(result.stdout)
        
        # Check for denials
        denials = opa_result.get("result", [{}])[0].get("deny", [])
        
        return {
            "allowed": len(denials) == 0,
            "violations": denials,
            "details": opa_result
        }
    
    def enforce_data_provisioning(self, request: Dict[str, Any]) -> bool:
        """
        Enforce policy before allowing data provisioning.
        Raises exception if policy violation detected.
        """
        policy_result = self.evaluate_policy(
            policy_file="governance/policies/test_data_governance.rego",
            input_data={
                "action": "provision_data",
                "masking_enabled": request.get("apply_masking", False),
                "volumes": request.get("volumes", {}),
                "environment": request.get("environment", "test"),
                "user_roles": request.get("user_roles", []),
                "rbac_enforced": True,
                "audit_logging_enabled": True,
                "encryption_enabled": True,
                "retention_days": request.get("retention_days", 90)
            }
        )
        
        if not policy_result["allowed"]:
            violations = "\n".join(policy_result["violations"])
            raise PermissionError(f"Policy violation(s):\n{violations}")
        
        return True
    
    def enforce_environment_creation(self, request: Dict[str, Any]) -> bool:
        """Enforce policy before environment creation"""
        policy_result = self.evaluate_policy(
            policy_file="governance/policies/test_data_governance.rego",
            input_data={
                "action": "create_env",
                "environment": request.get("environment", "test"),
                "user_roles": request.get("user_roles", []),
                "rbac_enforced": True,
                "credentials": request.get("credentials", {}),
                "audit_logging_enabled": True
            }
        )
        
        if not policy_result["allowed"]:
            violations = "\n".join(policy_result["violations"])
            raise PermissionError(f"Environment creation policy violation(s):\n{violations}")
        
        return True

# Usage in data provisioning pipeline
if __name__ == "__main__":
    enforcer = OPAPolicyEnforcer()
    
    # Request to provision data
    provision_request = {
        "datasets": ["users", "orders"],
        "volumes": {"users": 100, "orders": 500},
        "apply_masking": True,
        "environment": "test",
        "user_roles": ["qa-automation"],
        "retention_days": 30
    }
    
    try:
        enforcer.enforce_data_provisioning(provision_request)
        print("âœ“ Policy checks passed. Proceeding with data provisioning.")
    except PermissionError as e:
        print(f"âœ— Policy violation: {str(e)}")
```

---

## 5.5 Secrets Management

### 5.5.1 Automated Secret Rotation

```python
# governance/secret_management.py

from typing import Dict, Any
from datetime import datetime, timedelta
import os
import json

class SecretRotationManager:
    """Manages secure secret rotation for test environments"""
    
    def __init__(self, vault_endpoint: str, vault_token: str):
        """Initialize secret manager (Vault, AWS Secrets Manager, etc.)"""
        import hvac  # HashiCorp Vault
        self.vault_client = hvac.Client(url=vault_endpoint, token=vault_token)
        self.rotation_interval = timedelta(hours=24)
    
    def rotate_database_credentials(self, database_id: str) -> Dict[str, Any]:
        """
        Rotate database credentials with zero-downtime.
        
        Process:
        1. Generate new credentials
        2. Test connectivity
        3. Update all services with new credentials
        4. Revoke old credentials
        """
        print(f"ðŸ”„ Rotating credentials for database: {database_id}")
        
        # Get current credentials
        current_creds = self._get_secret(f"test-db/{database_id}/current")
        
        # Generate new credentials
        new_user = f"test-{datetime.now().strftime('%Y%m%d%H%M%S')}"
        new_password = self._generate_secure_password(32)
        
        # Create new DB user
        self._create_database_user(
            db_endpoint=current_creds["endpoint"],
            username=new_user,
            password=new_password,
            permissions=current_creds["permissions"]
        )
        
        # Test new credentials
        self._test_database_connection(
            db_endpoint=current_creds["endpoint"],
            username=new_user,
            password=new_password
        )
        
        # Store new credentials in Vault
        new_creds = {
            "endpoint": current_creds["endpoint"],
            "username": new_user,
            "password": new_password,
            "created_at": datetime.utcnow().isoformat(),
            "next_rotation": (datetime.utcnow() + self.rotation_interval).isoformat()
        }
        
        self._set_secret(f"test-db/{database_id}/current", new_creds)
        
        # Update services with new credentials
        self._update_service_credentials(database_id, new_creds)
        
        # Revoke old credentials
        old_user = current_creds.get("username")
        if old_user:
            self._revoke_database_user(
                db_endpoint=current_creds["endpoint"],
                username=old_user
            )
        
        # Audit log
        self._log_secret_rotation(database_id, old_user, new_user)
        
        print(f"âœ“ Credentials rotated successfully for {database_id}")
        return new_creds
    
    def rotate_api_keys(self, service_id: str) -> str:
        """Rotate API keys for test services"""
        new_api_key = self._generate_secure_password(64)
        
        # Store in Vault
        self._set_secret(f"api/{service_id}/key", {
            "key": new_api_key,
            "created_at": datetime.utcnow().isoformat(),
            "next_rotation": (datetime.utcnow() + self.rotation_interval).isoformat()
        })
        
        # Update service
        self._update_service_api_key(service_id, new_api_key)
        
        return new_api_key
    
    def schedule_rotation_checks(self):
        """Periodic task to check and rotate expiring secrets"""
        import schedule
        
        schedule.every(6).hours.do(self._check_and_rotate_secrets)
        
        print("âœ“ Secret rotation scheduler started")
    
    def _check_and_rotate_secrets(self):
        """Check all secrets for rotation"""
        secrets = self.vault_client.list_secret_paths("secret/test-")
        
        for secret_path in secrets:
            secret = self._get_secret(secret_path)
            
            if "next_rotation" in secret:
                next_rotation = datetime.fromisoformat(secret["next_rotation"])
                
                if datetime.utcnow() >= next_rotation:
                    # Rotation due
                    if "database" in secret_path:
                        database_id = secret_path.split("/")[-2]
                        self.rotate_database_credentials(database_id)
    
    def _get_secret(self, path: str) -> Dict[str, Any]:
        """Retrieve secret from Vault"""
        response = self.vault_client.read(f"secret/data/{path}")
        return response["data"]["data"]
    
    def _set_secret(self, path: str, data: Dict[str, Any]):
        """Store secret in Vault"""
        self.vault_client.write(f"secret/data/{path}", data=data)
    
    def _generate_secure_password(self, length: int) -> str:
        """Generate cryptographically secure password"""
        import secrets
        import string
        
        alphabet = string.ascii_letters + string.digits + "!@#$%^&*"
        return ''.join(secrets.choice(alphabet) for _ in range(length))
    
    def _create_database_user(self, db_endpoint: str, username: str, password: str, permissions: str):
        """Create new database user"""
        import psycopg2
        
        conn = psycopg2.connect(f"postgresql://admin:{os.getenv('ADMIN_DB_PASS')}@{db_endpoint}/postgres")
        cursor = conn.cursor()
        
        cursor.execute(f"CREATE USER {username} WITH PASSWORD %s", (password,))
        cursor.execute(f"GRANT {permissions} ON DATABASE testdb TO {username}")
        
        conn.commit()
        cursor.close()
        conn.close()
    
    def _revoke_database_user(self, db_endpoint: str, username: str):
        """Revoke and drop old database user"""
        import psycopg2
        
        conn = psycopg2.connect(f"postgresql://admin:{os.getenv('ADMIN_DB_PASS')}@{db_endpoint}/postgres")
        cursor = conn.cursor()
        
        cursor.execute(f"DROP USER IF EXISTS {username}")
        
        conn.commit()
        cursor.close()
        conn.close()
    
    def _test_database_connection(self, db_endpoint: str, username: str, password: str):
        """Test new credentials"""
        import psycopg2
        
        try:
            conn = psycopg2.connect(
                f"postgresql://{username}:{password}@{db_endpoint}/testdb"
            )
            cursor = conn.cursor()
            cursor.execute("SELECT 1")
            cursor.close()
            conn.close()
        except Exception as e:
            raise RuntimeError(f"Failed to verify new credentials: {str(e)}")
    
    def _update_service_credentials(self, service_id: str, new_creds: Dict):
        """Update all services using these credentials"""
        # Would update K8s secrets, environment variables, etc.
        pass
    
    def _update_service_api_key(self, service_id: str, new_key: str):
        """Update service with new API key"""
        # Would patch K8s deployment or update configuration
        pass
    
    def _log_secret_rotation(self, service: str, old_secret: str, new_secret: str):
        """Log secret rotation for audit"""
        print(f"[AUDIT] Secret rotated: {service}")
```

---

## Summary: Governance Automation Framework

**Key Capabilities**:
- âœ… Multi-layer PII masking (database + application)
- âœ… Role-based access control (RBAC) enforcement
- âœ… Immutable audit logging with chain-of-custody
- âœ… Policy-as-Code (OPA/Rego) enforcement
- âœ… Automated secret rotation (24-hour cycle)
- âœ… Compliance monitoring & reporting

**Business Impact**:
- ðŸ”’ 100% automated data protection
- ðŸ“‹ Audit-ready compliance (GDPR, SOC2, HIPAA)
- ðŸŽ¯ Zero manual governance steps
- ðŸš¨ Real-time policy violation detection
- âœ… Proof of compliance for auditors

---

**Next Document**: [06_OPERATING_MODEL.md](docs/06_OPERATING_MODEL.md)
