# 4. CI/CD INTEGRATION: ORCHESTRATING AUTOMATION

## 4.1 End-to-End Pipeline Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         DEVELOPER WORKFLOW                          â”‚
â”‚                    git push â†’ GitHub/GitLab                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              CI/CD PLATFORM (GitHub Actions / GitLab CI)            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â–¼                           â–¼                  â–¼                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STAGE 1:     â”‚  â”‚ STAGE 2:         â”‚  â”‚ STAGE 3:     â”‚  â”‚ STAGE 4:   â”‚
â”‚ PROVISION    â”‚  â”‚ VALIDATE         â”‚  â”‚ EXECUTE      â”‚  â”‚ REPORT &   â”‚
â”‚              â”‚  â”‚ ENVIRONMENT      â”‚  â”‚ TESTS        â”‚  â”‚ CLEANUP    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ â€¢ Terraform  â”‚  â”‚ â€¢ Health checks  â”‚  â”‚ â€¢ Run pytest â”‚  â”‚ â€¢ Aggregateâ”‚
â”‚ â€¢ Secrets    â”‚  â”‚ â€¢ Schema init    â”‚  â”‚ â€¢ Parallel   â”‚  â”‚ â€¢ Publish  â”‚
â”‚ â€¢ Test data  â”‚  â”‚ â€¢ Readiness      â”‚  â”‚ â€¢ Retry      â”‚  â”‚ â€¢ Destroy  â”‚
â”‚ â€¢ Networks   â”‚  â”‚ â€¢ Smoke tests    â”‚  â”‚ â€¢ Isolate    â”‚  â”‚ â€¢ Archive  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚                        â”‚                 â”‚
                     â”œâ”€ Failure â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
                     â”‚                                           â”‚
                     â–¼                                           â–¼
              â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                       â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
              â”‚ ROLLBACK INFRA  â”‚                       â”‚ FINAL TEARDOWN  â”‚
              â”‚ & DATA          â”‚                       â”‚ & AUDIT LOG     â”‚
              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                       â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Total Time: ~10 minutes for 50 parallel test jobs
```

---

## 4.2 GitHub Actions Pipeline

### 4.2.1 Main Workflow File

```yaml
# ci-cd-pipelines/github-actions/.github/workflows/test-automation.yml

name: Automated Test Suite

on:
  push:
    branches:
      - main
      - develop
      - 'feature/**'
  pull_request:
    branches:
      - main
      - develop
  schedule:
    # Nightly regression tests
    - cron: '0 2 * * *'

env:
  REGISTRY: ghcr.io
  IMAGE_TAG: ${{ github.sha }}
  PIPELINE_ID: ${{ github.run_id }}-${{ github.run_attempt }}
  AWS_REGION: us-east-1

jobs:
  # Stage 1: Provision Infrastructure & Test Data
  provision:
    name: Provision Test Environment
    runs-on: ubuntu-latest
    outputs:
      database_endpoint: ${{ steps.provision.outputs.database_endpoint }}
      database_name: ${{ steps.provision.outputs.database_name }}
      ecs_cluster: ${{ steps.provision.outputs.ecs_cluster }}
      vpc_id: ${{ steps.provision.outputs.vpc_id }}
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Configure AWS Credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          role-to-assume: ${{ secrets.AWS_ROLE_ARN }}
          aws-region: ${{ env.AWS_REGION }}
      
      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: 1.6.0
      
      - name: Initialize Terraform
        run: |
          cd test-environment-automation/terraform
          terraform init \
            -backend-config="bucket=${{ secrets.TF_STATE_BUCKET }}" \
            -backend-config="key=test/${{ env.PIPELINE_ID }}/terraform.tfstate" \
            -backend-config="region=${{ env.AWS_REGION }}"
      
      - name: Validate Terraform
        run: |
          cd test-environment-automation/terraform
          terraform validate
      
      - name: Plan Infrastructure
        run: |
          cd test-environment-automation/terraform
          terraform plan \
            -var="pipeline_id=${{ env.PIPELINE_ID }}" \
            -var="test_suite_name=e2e" \
            -out=tfplan
      
      - name: Apply Infrastructure
        id: provision
        run: |
          cd test-environment-automation/terraform
          terraform apply -auto-approve tfplan
          
          echo "database_endpoint=$(terraform output -raw database_endpoint)" >> $GITHUB_OUTPUT
          echo "database_name=$(terraform output -raw database_name)" >> $GITHUB_OUTPUT
          echo "ecs_cluster=$(terraform output -raw ecs_cluster_name)" >> $GITHUB_OUTPUT
          echo "vpc_id=$(terraform output -raw vpc_id)" >> $GITHUB_OUTPUT
      
      - name: Store Environment Details
        run: |
          mkdir -p /tmp/env-details
          cat > /tmp/env-details/env.json <<EOF
          {
            "pipeline_id": "${{ env.PIPELINE_ID }}",
            "database_endpoint": "${{ steps.provision.outputs.database_endpoint }}",
            "database_name": "${{ steps.provision.outputs.database_name }}",
            "ecs_cluster": "${{ steps.provision.outputs.ecs_cluster }}",
            "vpc_id": "${{ steps.provision.outputs.vpc_id }}",
            "provisioned_at": "$(date -u +%Y-%m-%dT%H:%M:%SZ)"
          }
          EOF
      
      - name: Upload Environment Details
        uses: actions/upload-artifact@v4
        with:
          name: environment-details
          path: /tmp/env-details/env.json
          retention-days: 1

  # Stage 2: Validate Environment & Provision Data
  validate-and-seed:
    name: Validate Environment & Seed Test Data
    needs: provision
    runs-on: ubuntu-latest
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install Dependencies
        run: |
          pip install -r test-data-automation/requirements.txt
          pip install -r test-environment-automation/requirements.txt
      
      - name: Configure Database Connection
        run: |
          cat > .env <<EOF
          DATABASE_HOST=${{ needs.provision.outputs.database_endpoint }}
          DATABASE_NAME=${{ needs.provision.outputs.database_name }}
          DATABASE_USER=${{ secrets.TEST_DB_USER }}
          DATABASE_PASSWORD=${{ secrets.TEST_DB_PASSWORD }}
          API_URL=http://api:3000
          REDIS_URL=redis://redis:6379
          ENVIRONMENT=test
          EOF
      
      - name: Run Environment Health Checks
        run: |
          python test-environment-automation/health_checks.py \
            --database-url postgresql://${{ secrets.TEST_DB_USER }}:${{ secrets.TEST_DB_PASSWORD }}@${{ needs.provision.outputs.database_endpoint }}:5432/${{ needs.provision.outputs.database_name }} \
            --api-url http://localhost:3000 \
            --timeout 120 \
            --retry-count 5
      
      - name: Initialize Database Schema
        run: |
          python test-data-automation/schema_init.py \
            --db-url postgresql://${{ secrets.TEST_DB_USER }}:${{ secrets.TEST_DB_PASSWORD }}@${{ needs.provision.outputs.database_endpoint }}:5432/${{ needs.provision.outputs.database_name }} \
            --schema-dir test-data-automation/schemas/
      
      - name: Provision Test Data
        run: |
          python -c "
          from test_data_automation.data_provision_api import DataProvisioningService
          from pydantic import BaseModel
          
          service = DataProvisioningService({
            'host': '${{ needs.provision.outputs.database_endpoint }}',
            'database': '${{ needs.provision.outputs.database_name }}',
            'user': '${{ secrets.TEST_DB_USER }}',
            'password': '${{ secrets.TEST_DB_PASSWORD }}'
          })
          
          result = service.provision({
            'datasets': ['users', 'orders', 'payments'],
            'volumes': {'users': 100, 'orders': 500, 'payments': 500},
            'apply_masking': True,
            'version': 'latest'
          })
          
          print(f'âœ“ Provisioned: {result}')
          "
      
      - name: Run Smoke Tests
        run: |
          pytest test-data-automation/tests/test_data_integrity.py \
            -v --tb=short --maxfail=3
      
      - name: Upload Validation Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: validation-report
          path: reports/validation/
          retention-days: 7

  # Stage 3: Run Tests in Parallel
  test:
    name: Execute Test Suite (Parallel)
    needs: [provision, validate-and-seed]
    runs-on: ubuntu-latest
    strategy:
      matrix:
        test-suite:
          - name: e2e-checkout
            path: tests/e2e/checkout
            timeout: 600
          - name: e2e-account
            path: tests/e2e/account
            timeout: 600
          - name: api-integration
            path: tests/integration/api
            timeout: 600
          - name: data-isolation
            path: tests/integration/data-isolation
            timeout: 600
          - name: performance
            path: tests/performance
            timeout: 900
      max-parallel: 5
      fail-fast: false
    
    timeout-minutes: 20
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'
      
      - name: Install Test Dependencies
        run: |
          pip install -r requirements-test.txt
          pip install pytest pytest-xdist pytest-cov pytest-timeout
      
      - name: Configure Test Environment
        run: |
          cat > .env <<EOF
          DATABASE_HOST=${{ needs.provision.outputs.database_endpoint }}
          DATABASE_NAME=${{ needs.provision.outputs.database_name }}
          DATABASE_USER=${{ secrets.TEST_DB_USER }}
          DATABASE_PASSWORD=${{ secrets.TEST_DB_PASSWORD }}
          API_URL=http://api:3000
          REDIS_URL=redis://redis:6379
          PIPELINE_ID=${{ env.PIPELINE_ID }}
          TEST_SUITE=${{ matrix.test-suite.name }}
          ENVIRONMENT=test
          EOF
      
      - name: Run Tests - ${{ matrix.test-suite.name }}
        id: test-run
        timeout-minutes: ${{ matrix.test-suite.timeout / 60 }}
        run: |
          pytest ${{ matrix.test-suite.path }} \
            -v \
            --tb=short \
            --junit-xml=reports/junit-${{ matrix.test-suite.name }}.xml \
            --cov=src \
            --cov-report=html:reports/coverage-${{ matrix.test-suite.name }} \
            --cov-report=term \
            --maxfail=5 \
            --timeout=${{ matrix.test-suite.timeout }} \
            --durations=10 \
            -n auto
        continue-on-error: true
      
      - name: Retry Failed Tests (once)
        if: steps.test-run.outcome == 'failure'
        run: |
          pytest ${{ matrix.test-suite.path }} \
            -v \
            --tb=short \
            --junit-xml=reports/junit-${{ matrix.test-suite.name }}-retry.xml \
            --lf \
            --maxfail=3 \
            --timeout=${{ matrix.test-suite.timeout }}
        continue-on-error: true
      
      - name: Collect Test Artifacts
        if: always()
        run: |
          mkdir -p reports/${{ matrix.test-suite.name }}
          cp -r logs/ reports/${{ matrix.test-suite.name }}/ || true
          cp -r screenshots/ reports/${{ matrix.test-suite.name }}/ || true
      
      - name: Upload Test Results
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: test-results-${{ matrix.test-suite.name }}
          path: reports/
          retention-days: 30

  # Stage 4: Aggregate Results & Cleanup
  report-and-cleanup:
    name: Generate Reports & Clean Up
    needs: [provision, test]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Checkout Code
        uses: actions/checkout@v4
      
      - name: Download All Artifacts
        uses: actions/download-artifact@v4
        with:
          path: aggregated-reports/
      
      - name: Generate Test Report
        run: |
          python ci-cd-pipelines/report_generator.py \
            --reports-dir aggregated-reports/ \
            --output-file test-report.html
      
      - name: Calculate Test Metrics
        id: metrics
        run: |
          python ci-cd-pipelines/metrics_calculator.py \
            --reports-dir aggregated-reports/
          
          echo "total_tests=$(cat metrics.json | jq .total_tests)" >> $GITHUB_OUTPUT
          echo "passed_tests=$(cat metrics.json | jq .passed_tests)" >> $GITHUB_OUTPUT
          echo "failed_tests=$(cat metrics.json | jq .failed_tests)" >> $GITHUB_OUTPUT
          echo "pass_rate=$(cat metrics.json | jq .pass_rate)" >> $GITHUB_OUTPUT
      
      - name: Post Test Results Comment
        if: github.event_name == 'pull_request'
        uses: actions/github-script@v7
        with:
          script: |
            const fs = require('fs');
            const metrics = JSON.parse(fs.readFileSync('metrics.json', 'utf8'));
            
            const comment = `## ðŸ§ª Test Automation Results
            
            | Metric | Value |
            |--------|-------|
            | **Total Tests** | ${{ steps.metrics.outputs.total_tests }} |
            | **Passed** | ${{ steps.metrics.outputs.passed_tests }} |
            | **Failed** | ${{ steps.metrics.outputs.failed_tests }} |
            | **Pass Rate** | ${{ steps.metrics.outputs.pass_rate }} |
            
            **Status**: ${metrics.overall_status === 'SUCCESS' ? 'âœ… All tests passed' : 'âŒ Some tests failed'}
            
            [View Detailed Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})
            `;
            
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: comment
            });
      
      - name: Archive Logs
        if: always()
        run: |
          aws configure set aws_access_key_id ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws configure set aws_secret_access_key ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws configure set region ${{ env.AWS_REGION }}
          
          tar czf test-logs-${{ env.PIPELINE_ID }}.tar.gz aggregated-reports/
          aws s3 cp test-logs-${{ env.PIPELINE_ID }}.tar.gz \
            s3://${{ secrets.LOGS_BUCKET }}/pipeline/${{ env.PIPELINE_ID }}/
      
      - name: Cleanup Test Environment (Success)
        if: success()
        run: |
          bash test-environment-automation/teardown_environment.sh \
            ${{ env.PIPELINE_ID }} \
            false
      
      - name: Cleanup Test Environment (Failure - Preserve for Debug)
        if: failure()
        run: |
          echo "âš ï¸ Tests failed. Preserving environment for debugging."
          bash test-environment-automation/teardown_environment.sh \
            ${{ env.PIPELINE_ID }} \
            true
          
          echo "Environment preserved. Debug instructions:"
          echo "  1. SSH into environment: aws ec2 describe-instances --filters Name=tag:Pipeline,Values=${{ env.PIPELINE_ID }}"
          echo "  2. Check logs: aws logs tail /aws/ecs/test-${{ env.PIPELINE_ID }} --follow"
          echo "  3. Cleanup when done: terraform destroy -var pipeline_id=${{ env.PIPELINE_ID }}"
      
      - name: Update Status in Database
        if: always()
        run: |
          python ci-cd-pipelines/update_execution_log.py \
            --pipeline-id ${{ env.PIPELINE_ID }} \
            --status ${{ job.status }} \
            --total-tests ${{ steps.metrics.outputs.total_tests }} \
            --passed-tests ${{ steps.metrics.outputs.passed_tests }} \
            --duration ${{ github.run_number }}
      
      - name: Fail Job if Tests Failed
        if: steps.metrics.outputs.pass_rate != '100'
        run: |
          echo "âŒ Tests did not meet 100% pass rate"
          exit 1
      
      - name: Upload Final Report
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: final-test-report
          path: |
            test-report.html
            metrics.json
          retention-days: 90

  # Status notifications
  notify:
    name: Send Notifications
    needs: [report-and-cleanup]
    runs-on: ubuntu-latest
    if: always()
    
    steps:
      - name: Determine Status
        id: status
        run: |
          if [ "${{ needs.report-and-cleanup.result }}" == "success" ]; then
            echo "emoji=âœ…" >> $GITHUB_OUTPUT
            echo "status=PASSED" >> $GITHUB_OUTPUT
          else
            echo "emoji=âŒ" >> $GITHUB_OUTPUT
            echo "status=FAILED" >> $GITHUB_OUTPUT
          fi
      
      - name: Slack Notification
        uses: slackapi/slack-github-action@v1.24.0
        with:
          webhook-url: ${{ secrets.SLACK_WEBHOOK }}
          payload: |
            {
              "text": "${{ steps.status.outputs.emoji }} Test Automation ${{ steps.status.outputs.status }}",
              "blocks": [
                {
                  "type": "section",
                  "text": {
                    "type": "mrkdwn",
                    "text": "*Test Automation Results*\n*Status*: ${{ steps.status.outputs.status }}\n*Branch*: ${{ github.ref }}\n*Commit*: ${{ github.sha }}"
                  }
                },
                {
                  "type": "actions",
                  "elements": [
                    {
                      "type": "button",
                      "text": {
                        "type": "plain_text",
                        "text": "View Details"
                      },
                      "url": "https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                    }
                  ]
                }
              ]
            }
```

---

## 4.3 GitLab CI Pipeline

```yaml
# ci-cd-pipelines/gitlab-ci/.gitlab-ci.yml

stages:
  - provision
  - validate
  - test
  - report
  - cleanup

variables:
  PIPELINE_ID: "${CI_PIPELINE_ID}-${CI_JOB_ID}"
  AWS_REGION: "us-east-1"
  TF_ROOT: "${CI_PROJECT_DIR}/test-environment-automation/terraform"

# Base job definition
.env_base:
  image: python:3.11
  before_script:
    - pip install --upgrade pip
    - pip install -r requirements-ci.txt

# Stage 1: Provision
provision:
  stage: provision
  extends: .env_base
  image: hashicorp/terraform:latest
  script:
    - cd ${TF_ROOT}
    - terraform init
    - terraform plan -var="pipeline_id=${PIPELINE_ID}" -out=tfplan
    - terraform apply -auto-approve tfplan
    - |
      terraform output -json > /tmp/env-details.json
  artifacts:
    paths:
      - /tmp/env-details.json
    expire_in: 1 hour
  retry:
    max: 2
    when:
      - api_failure
      - runner_system_failure

# Stage 2: Validate Environment
validate:
  stage: validate
  extends: .env_base
  dependencies:
    - provision
  script:
    - |
      ENV_DETAILS=$(cat /tmp/env-details.json)
      DB_ENDPOINT=$(echo $ENV_DETAILS | jq -r '.database_endpoint.value')
      
      python test-environment-automation/health_checks.py \
        --database-url "postgresql://${TEST_DB_USER}:${TEST_DB_PASSWORD}@${DB_ENDPOINT}:5432/testdb" \
        --timeout 120
    
    - python test-data-automation/schema_init.py
    - pytest test-data-automation/tests/test_data_integrity.py -v
  artifacts:
    paths:
      - reports/validation/
    expire_in: 7 days

# Stage 3: Run Tests (Parallel Jobs)
test:e2e:checkout:
  stage: test
  extends: .env_base
  script:
    - pytest tests/e2e/checkout -v --junit-xml=reports/junit-e2e-checkout.xml
  artifacts:
    reports:
      junit: reports/junit-e2e-checkout.xml
    paths:
      - reports/
    expire_in: 30 days

test:e2e:account:
  stage: test
  extends: .env_base
  script:
    - pytest tests/e2e/account -v --junit-xml=reports/junit-e2e-account.xml
  artifacts:
    reports:
      junit: reports/junit-e2e-account.xml
    paths:
      - reports/
    expire_in: 30 days

test:integration:api:
  stage: test
  extends: .env_base
  script:
    - pytest tests/integration/api -v --junit-xml=reports/junit-integration-api.xml
  artifacts:
    reports:
      junit: reports/junit-integration-api.xml
    paths:
      - reports/
    expire_in: 30 days

test:performance:
  stage: test
  extends: .env_base
  script:
    - pytest tests/performance -v --junit-xml=reports/junit-performance.xml
  artifacts:
    reports:
      junit: reports/junit-performance.xml
    paths:
      - reports/
    expire_in: 30 days

# Stage 4: Report & Analysis
report:
  stage: report
  extends: .env_base
  dependencies:
    - test:e2e:checkout
    - test:e2e:account
    - test:integration:api
    - test:performance
  script:
    - python ci-cd-pipelines/report_generator.py --reports-dir reports/
    - python ci-cd-pipelines/metrics_calculator.py --reports-dir reports/
  artifacts:
    paths:
      - test-report.html
      - metrics.json
    expire_in: 90 days
  allow_failure: true

# Stage 5: Cleanup
cleanup:success:
  stage: cleanup
  extends: .env_base
  image: hashicorp/terraform:latest
  script:
    - cd ${TF_ROOT}
    - terraform init
    - terraform destroy -auto-approve -var="pipeline_id=${PIPELINE_ID}"
  when: on_success

cleanup:failure:
  stage: cleanup
  extends: .env_base
  image: hashicorp/terraform:latest
  script:
    - echo "âš ï¸ Tests failed. Preserving environment for debugging."
    - echo "Pipeline ID: ${PIPELINE_ID}"
    - echo "Cleanup when done: terraform destroy -var pipeline_id=${PIPELINE_ID}"
  when: on_failure
  allow_failure: true
```

---

## 4.4 Pipeline Orchestration Patterns

### 4.4.1 Parallel Execution Controller

```python
# ci-cd-pipelines/orchestration.py

from typing import List, Dict, Any
import subprocess
import json
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging

logger = logging.getLogger(__name__)

@dataclass
class TestJob:
    name: str
    test_path: str
    timeout: int = 600
    retry_count: int = 1
    max_workers: int = 5

class PipelineOrchestrator:
    """Orchestrates parallel test execution across multiple jobs"""
    
    def __init__(self, environment_config: Dict[str, Any]):
        self.env_config = environment_config
        self.job_results = {}
    
    def execute_parallel(self, jobs: List[TestJob]) -> Dict[str, Any]:
        """
        Execute multiple test jobs in parallel with orchestration.
        
        Returns aggregated results.
        """
        logger.info(f"Starting parallel execution of {len(jobs)} jobs")
        
        with ThreadPoolExecutor(max_workers=jobs[0].max_workers) as executor:
            future_to_job = {
                executor.submit(self._execute_job, job): job 
                for job in jobs
            }
            
            for future in as_completed(future_to_job):
                job = future_to_job[future]
                try:
                    result = future.result()
                    self.job_results[job.name] = result
                    logger.info(f"âœ“ {job.name}: {result['status']}")
                except Exception as e:
                    logger.error(f"âœ— {job.name}: {str(e)}")
                    self.job_results[job.name] = {
                        "status": "failed",
                        "error": str(e)
                    }
        
        return self._aggregate_results()
    
    def _execute_job(self, job: TestJob) -> Dict[str, Any]:
        """Execute a single test job with retries"""
        for attempt in range(job.retry_count):
            logger.info(f"Executing {job.name} (attempt {attempt + 1}/{job.retry_count})")
            
            result = self._run_pytest(job.test_path, job.timeout)
            
            if result['status'] == 'passed':
                return result
            
            logger.warning(f"{job.name} failed, retrying...")
        
        return result
    
    def _run_pytest(self, test_path: str, timeout: int) -> Dict[str, Any]:
        """Run pytest and capture results"""
        cmd = [
            "pytest",
            test_path,
            "--junit-xml=reports/junit.xml",
            "--cov=src",
            "--cov-report=html",
            f"--timeout={timeout}",
            "-v"
        ]
        
        try:
            result = subprocess.run(
                cmd,
                capture_output=True,
                text=True,
                timeout=timeout + 60
            )
            
            return {
                "status": "passed" if result.returncode == 0 else "failed",
                "return_code": result.returncode,
                "stdout": result.stdout[-1000:],  # Last 1000 chars
                "stderr": result.stderr[-1000:]
            }
        except subprocess.TimeoutExpired:
            return {
                "status": "timeout",
                "error": f"Test exceeded {timeout}s timeout"
            }
    
    def _aggregate_results(self) -> Dict[str, Any]:
        """Aggregate results from all jobs"""
        total = len(self.job_results)
        passed = sum(1 for r in self.job_results.values() if r['status'] == 'passed')
        failed = sum(1 for r in self.job_results.values() if r['status'] == 'failed')
        
        return {
            "total_jobs": total,
            "passed_jobs": passed,
            "failed_jobs": failed,
            "overall_status": "success" if failed == 0 else "failure",
            "details": self.job_results
        }

# Usage
if __name__ == "__main__":
    jobs = [
        TestJob(name="e2e-checkout", test_path="tests/e2e/checkout"),
        TestJob(name="e2e-account", test_path="tests/e2e/account"),
        TestJob(name="integration-api", test_path="tests/integration/api"),
        TestJob(name="performance", test_path="tests/performance", timeout=900),
    ]
    
    orchestrator = PipelineOrchestrator(env_config={})
    results = orchestrator.execute_parallel(jobs)
    
    print(json.dumps(results, indent=2))
```

---

## 4.5 Failure Recovery & Resilience

### 4.5.1 Automatic Retry with Backoff

```python
# ci-cd-pipelines/resilience.py

import time
import random
from typing import Callable, Any
from functools import wraps
import logging

logger = logging.getLogger(__name__)

def retry_with_backoff(
    max_retries: int = 3,
    initial_delay: float = 1.0,
    max_delay: float = 60.0,
    exponential_base: float = 2.0,
    jitter: bool = True
):
    """
    Decorator for automatic retry with exponential backoff.
    
    Usage:
    @retry_with_backoff(max_retries=3)
    def flaky_test_operation():
        ...
    """
    def decorator(func: Callable) -> Callable:
        @wraps(func)
        def wrapper(*args, **kwargs) -> Any:
            delay = initial_delay
            
            for attempt in range(max_retries):
                try:
                    logger.info(f"Attempt {attempt + 1}/{max_retries}: {func.__name__}")
                    return func(*args, **kwargs)
                
                except Exception as e:
                    if attempt == max_retries - 1:
                        logger.error(f"All {max_retries} attempts failed: {str(e)}")
                        raise
                    
                    # Calculate backoff with jitter
                    if jitter:
                        delay = delay * (1 + random.random())
                    else:
                        delay = delay * exponential_base
                    
                    delay = min(delay, max_delay)
                    
                    logger.warning(
                        f"Attempt {attempt + 1} failed: {str(e)}. "
                        f"Retrying in {delay:.1f}s..."
                    )
                    time.sleep(delay)
        
        return wrapper
    return decorator

# Example usage in test
@retry_with_backoff(max_retries=3)
def test_checkout_with_retry():
    """Test that retries on failure"""
    response = api_client.checkout(user_id=1, order_id=1)
    assert response.status_code == 200
```

### 4.5.2 Failure Diagnostics & Auto-Rollback

```python
# ci-cd-pipelines/failure_handler.py

from typing import Dict, Any
import logging
from datetime import datetime

logger = logging.getLogger(__name__)

class FailureHandler:
    """Handles pipeline failures and initiates recovery"""
    
    def __init__(self, pipeline_id: str, terraform_dir: str):
        self.pipeline_id = pipeline_id
        self.terraform_dir = terraform_dir
        self.failure_log = []
    
    def handle_test_failure(self, job_name: str, error: Exception, logs: str):
        """
        Analyze test failure and attempt recovery.
        """
        logger.error(f"Test failure in {job_name}: {str(error)}")
        
        # Categorize failure
        failure_type = self._categorize_failure(error, logs)
        
        # Log failure
        self.failure_log.append({
            "timestamp": datetime.utcnow().isoformat(),
            "job": job_name,
            "failure_type": failure_type,
            "error": str(error),
            "recovery_attempted": False
        })
        
        # Attempt recovery if data-related
        if failure_type == "data_integrity":
            logger.info("Attempting data recovery...")
            self._recover_data()
        
        # Attempt recovery if environment-related
        elif failure_type == "environment_connectivity":
            logger.info("Attempting environment reset...")
            self._reset_environment()
        
        return self.failure_log[-1]
    
    def _categorize_failure(self, error: Exception, logs: str) -> str:
        """Categorize failure type"""
        error_str = str(error).lower()
        logs_str = logs.lower()
        
        if any(x in error_str or x in logs_str for x in ["connection", "timeout", "refused"]):
            return "environment_connectivity"
        
        elif any(x in error_str or x in logs_str for x in ["constraint", "integrity", "foreign key"]):
            return "data_integrity"
        
        elif any(x in error_str or x in logs_str for x in ["assertion", "expected", "actual"]):
            return "test_logic"
        
        else:
            return "unknown"
    
    def _recover_data(self):
        """Attempt data recovery"""
        logger.info("Re-seeding test data...")
        # Implementation would call data provisioning API
        pass
    
    def _reset_environment(self):
        """Reset environment and retry"""
        logger.info("Resetting test environment...")
        import subprocess
        
        try:
            # Bounce ECS cluster
            subprocess.run([
                "aws", "ecs", "update-service",
                "--cluster", f"test-cluster-{self.pipeline_id}",
                "--service", "api",
                "--force-new-deployment"
            ])
            logger.info("Environment reset triggered")
        except Exception as e:
            logger.error(f"Failed to reset environment: {str(e)}")
    
    def generate_failure_report(self) -> Dict[str, Any]:
        """Generate failure report for debugging"""
        return {
            "pipeline_id": self.pipeline_id,
            "total_failures": len(self.failure_log),
            "failures": self.failure_log,
            "recommendations": self._generate_recommendations()
        }
    
    def _generate_recommendations(self) -> list:
        """Generate debugging recommendations"""
        recommendations = []
        
        failure_types = [f["failure_type"] for f in self.failure_log]
        
        if "environment_connectivity" in failure_types:
            recommendations.append(
                "Check AWS/infrastructure health: "
                "terraform plan -var pipeline_id={self.pipeline_id}"
            )
        
        if "data_integrity" in failure_types:
            recommendations.append(
                "Verify data constraints and referential integrity"
            )
        
        return recommendations
```

---

## 4.6 Repository Structure

```
ci-cd-pipelines/
â”œâ”€â”€ github-actions/
â”‚   â”œâ”€â”€ .github/workflows/
â”‚   â”‚   â”œâ”€â”€ test-automation.yml
â”‚   â”‚   â”œâ”€â”€ nightly-regression.yml
â”‚   â”‚   â””â”€â”€ manual-test-run.yml
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ provision.sh
â”‚       â”œâ”€â”€ teardown.sh
â”‚       â””â”€â”€ validate.sh
â”‚
â”œâ”€â”€ gitlab-ci/
â”‚   â”œâ”€â”€ .gitlab-ci.yml
â”‚   â””â”€â”€ scripts/
â”‚       â”œâ”€â”€ provision.sh
â”‚       â”œâ”€â”€ validate.sh
â”‚       â””â”€â”€ cleanup.sh
â”‚
â”œâ”€â”€ jenkins/
â”‚   â”œâ”€â”€ Jenkinsfile
â”‚   â”œâ”€â”€ Jenkinsfile.declarative
â”‚   â””â”€â”€ groovy-scripts/
â”‚
â”œâ”€â”€ orchestration.py         # Parallel execution controller
â”œâ”€â”€ resilience.py            # Retry logic & backoff
â”œâ”€â”€ failure_handler.py       # Failure recovery
â”œâ”€â”€ report_generator.py      # HTML report generation
â”œâ”€â”€ metrics_calculator.py    # Test metrics aggregation
â”œâ”€â”€ update_execution_log.py  # Log execution results
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ github-actions-example.yml
â”‚   â”œâ”€â”€ gitlab-ci-example.yml
â”‚   â””â”€â”€ jenkins-example.groovy
â”‚
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## Summary: CI/CD Integration Framework

**Key Capabilities**:
- âœ… GitHub Actions, GitLab CI, Jenkins templates
- âœ… Parallel test execution (50+ jobs)
- âœ… Automatic provisioning & teardown
- âœ… Failure recovery with retries
- âœ… Aggregated reporting and metrics
- âœ… Slack/email notifications
- âœ… Cost-optimized resource cleanup

**Business Impact**:
- ðŸš€ 10-minute end-to-end pipeline (vs. 4+ hours manual)
- âœ… Zero manual infrastructure steps
- ðŸ“Š Complete visibility into test results
- ðŸ”„ Automatic failure recovery
- ðŸ’° Guaranteed cleanup â†’ cost control

---

**Next Document**: [05_GOVERNANCE_AUTOMATION.md](docs/05_GOVERNANCE_AUTOMATION.md)
